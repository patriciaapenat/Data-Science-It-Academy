{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776612aa",
   "metadata": {},
   "source": [
    "# Tasca M9T01, Anàlisi de sentiment i textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b761bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27853f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurar els gràfics\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('flare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "812ae132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# download nltk corpus (first time only)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf2ad6",
   "metadata": {},
   "source": [
    "### Càrrega de l'arxiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "acc34357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "with open('C:/Users/Patricia/Desktop/prideandprejudice.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63315e",
   "metadata": {},
   "source": [
    "Pel anàlisi de sentiment farè servir un pdf que conté \"Orgull i prejudici\" (en anglès, \"Pride and Prejudice\") és una novel·la escrita per Jane Austen. Publicada per primera vegada el 1813, la història es desenvolupa a la zona rural d'Anglaterra al segle XIX i segueix les vivències de la família Bennet. L'eix central de la trama és la relació entre Elizabeth Bennet, una dona intel·ligent i vivaç, i Fitzwilliam Darcy, un home orgullós i aparentment arrogant.\n",
    "\n",
    "La novel·la explora temes com el matrimoni, la societat i les expectatives de gènere de l'època. Mitjançant diàlegs perspicaços i situacions còmiques, Austen retrata les complexitats de les relacions humanes i critica la rigidesa social i la importància excessiva que es donava a l'estatus i els diners en la societat de l'època.\n",
    "\n",
    "Al llarg de la història, Elizabeth ha d'enfrontar-se als seus prejudicis i superar les seves pròpies concepcions errònies sobre el caràcter de Darcy. Al mateix temps, Darcy ha d'aprendre a deixar de banda el seu orgull i prejudici inicials i reconèixer el veritable valor d'Elizabeth. Junts, Elizabeth i Darcy descobreixen l'amor i la capacitat de transcendir les expectatives i convencions socials.\n",
    "\n",
    "    També és de les primeres obres en tenir el clixé \"enemies to lovers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376042b1",
   "metadata": {},
   "source": [
    "### Preprocessat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec022f3",
   "metadata": {},
   "source": [
    "Primer dividim el text per capitols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9ba6e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Chapter                                            Content\n",
      "0         1  It is a truth universally acknowledged, that a...\n",
      "1         2  Mr. Bennet was among the earliest of those who...\n",
      "2         3  Not all that Mrs. Bennet, however, with the as...\n",
      "3         4  When Jane and Elizabeth were alone, the former...\n",
      "4         5  Within a short walk of Longbourn lived a famil...\n",
      "..      ...                                                ...\n",
      "56       57  The discomposure of spirits which this extraor...\n",
      "57       58  Instead of receiving any such letter of excuse...\n",
      "58       59  “My dear Lizzy, where can you have been walkin...\n",
      "59       60  Elizabeth’s spirits soon rising to playfulness...\n",
      "60       61  Happy for all her maternal feelings was the da...\n",
      "\n",
      "[61 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Patró de coincidència per extreure els marcadors de capítol i el contingut\n",
    "pattern = r\"Chapter (\\d+)\\n([\\s\\S]*?)(?=(Chapter \\d+|$))\"\n",
    "\n",
    "# Troba tots els marcadors de capítol i el contingut amb el patró\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "# Crea una llista de capítols amb el número de capítol i el contingut\n",
    "chapters = [(int(match[0]), match[1].strip()) for match in matches]\n",
    "\n",
    "# Crea un DataFrame amb els capítols\n",
    "df = pd.DataFrame(chapters, columns=[\"Chapter\", \"Content\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b299b64",
   "metadata": {},
   "source": [
    "Com que el text selecionat és una novel·la caldria identificar i eliminar el soroll, així com normalitzar els caràcters sense tenir fer-li masking a les nostres dades. Com que és un pdf també caldrà revisar si hi ha etiquetes HTML o URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "353763f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def normalize_text(df, column):\n",
    "    # Passos d'eliminació de soroll i normalització de caràcters\n",
    "    passos = [\n",
    "        (r'<[^<]+?>', '', 'Etiquetes HTML'),      # Eliminació d'etiquetes HTML\n",
    "        (r'http\\S+', '', 'URLs'),                  # Eliminació d'URLs\n",
    "        (r'[^\\w\\s]', '', 'puntuacio'),             # Eliminació de símbols de puntuació\n",
    "        (r'\\d+', '', 'nombres'),                    # Eliminació de nombres\n",
    "        (None, lambda x: x.lower(), ''),           # Conversió a minúscules\n",
    "        (None, lambda x: unidecode(x), '')         # Eliminació d'accentuació i diacrítics\n",
    "    ]\n",
    "\n",
    "    # Aplicació dels passos al text\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[column]\n",
    "        for patro, substitucio, etiqueta in passos:\n",
    "            if patro:\n",
    "                text = re.sub(patro, substitucio, text)\n",
    "            elif substitucio:\n",
    "                text = substitucio(text)\n",
    "        df.at[index, column] = text\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2c5f56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Chapter                                            Content\n",
      "0         1  it is a truth universally acknowledged that a ...\n",
      "1         2  mr bennet was among the earliest of those who ...\n",
      "2         3  not all that mrs bennet however with the assis...\n",
      "3         4  when jane and elizabeth were alone the former ...\n",
      "4         5  within a short walk of longbourn lived a famil...\n",
      "..      ...                                                ...\n",
      "56       57  the discomposure of spirits which this extraor...\n",
      "57       58  instead of receiving any such letter of excuse...\n",
      "58       59  my dear lizzy where can you have been walking ...\n",
      "59       60  elizabeths spirits soon rising to playfulness ...\n",
      "60       61  happy for all her maternal feelings was the da...\n",
      "\n",
      "[61 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "normalized = normalize_text(df, 'Content')\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5d95a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Inicialització del vectoritzador TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorització de la columna 'Content' de la taula normalized per obtenir la matriu term-document\n",
    "matriu_term_document = vectorizer.fit_transform(normalized['Content'])\n",
    "\n",
    "# Llista de termes (columnes) de la matriu term-document\n",
    "termes = list(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea258aab",
   "metadata": {},
   "source": [
    "with open('C:/Users/Patricia/Desktop/prideandprejudice/prideandprejudice.txt', 'w') as archivo:\n",
    "    for columna in termes:\n",
    "        archivo.write(columna + ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a063f7b",
   "metadata": {},
   "source": [
    "##### Ara si fariem el preprocessat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c289013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Crear función preprocess_text con stemming\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_tokens = [PorterStemmer().stem(lemmatizer.lemmatize(token)) for token in filtered_tokens]\n",
    "    processed_text = ' '.join(stemmed_tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "08f0a026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>it is a truth universally acknowledged that a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>mr bennet was among the earliest of those who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>not all that mrs bennet however with the assis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>when jane and elizabeth were alone the former ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>within a short walk of longbourn lived a famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>the discomposure of spirits which this extraor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>instead of receiving any such letter of excuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>my dear lizzy where can you have been walking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>elizabeths spirits soon rising to playfulness ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>happy for all her maternal feelings was the da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Chapter                                            Content\n",
       "0         1  it is a truth universally acknowledged that a ...\n",
       "1         2  mr bennet was among the earliest of those who ...\n",
       "2         3  not all that mrs bennet however with the assis...\n",
       "3         4  when jane and elizabeth were alone the former ...\n",
       "4         5  within a short walk of longbourn lived a famil...\n",
       "..      ...                                                ...\n",
       "56       57  the discomposure of spirits which this extraor...\n",
       "57       58  instead of receiving any such letter of excuse...\n",
       "58       59  my dear lizzy where can you have been walking ...\n",
       "59       60  elizabeths spirits soon rising to playfulness ...\n",
       "60       61  happy for all her maternal feelings was the da...\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized['Content'] = normalized['Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0f768c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text and remove common words\n",
    "processed = normalized['Chapter'], normalized['Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6757844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultats de LSA:\n",
      "[[ 7.21676619e-01  3.96945540e-01  1.24751497e-01  3.33803254e-02\n",
      "  -1.43514809e-02 -1.77103496e-02  3.10801158e-01  1.00407788e-01\n",
      "   1.70887463e-01 -1.15026763e-01]\n",
      " [ 7.56716104e-01  2.92989137e-01  5.32954812e-02 -7.98415592e-02\n",
      "   1.91427183e-01 -6.51728461e-02  1.82899489e-01  1.18320601e-01\n",
      "   1.96920554e-01  1.91401802e-01]\n",
      " [ 8.41735793e-01 -1.06051137e-01  2.47017482e-01 -5.23061972e-02\n",
      "   1.25919800e-01  5.60619017e-02  2.78723217e-02 -3.49739073e-02\n",
      "   1.12529230e-01  4.28861339e-02]\n",
      " [ 8.12646859e-01 -5.40285212e-02  1.98583083e-01 -5.01543615e-02\n",
      "  -3.64706505e-03  6.17833851e-02  1.18408217e-01 -2.58518618e-01\n",
      "   4.72238240e-02 -1.07335900e-01]\n",
      " [ 7.36654295e-01  1.05145655e-01  3.89032340e-01 -2.49605931e-01\n",
      "   2.07200186e-02  2.03435308e-01 -2.40640338e-01  2.07421454e-03\n",
      "  -2.18387689e-02 -7.14826666e-03]\n",
      " [ 8.97534192e-01  7.18568686e-02  5.73537587e-02 -6.01430394e-02\n",
      "   6.67893353e-03 -6.93988348e-02 -3.84064524e-02 -7.75318487e-02\n",
      "  -2.93761948e-02 -4.62305323e-02]\n",
      " [ 8.73789746e-01 -4.16582765e-02 -1.27365865e-01  2.29710569e-02\n",
      "   1.46400347e-01  9.31052645e-02  7.00800883e-02 -3.50422652e-02\n",
      "   4.29335990e-02 -8.80363038e-02]\n",
      " [ 8.33166421e-01  3.18364227e-02 -1.06742627e-01 -1.26198883e-01\n",
      "   1.52126437e-01 -1.53260103e-01 -4.98243693e-02 -1.37596023e-01\n",
      "  -4.42492583e-03 -1.42941605e-01]\n",
      " [ 8.53115025e-01  7.39241261e-02 -2.46078532e-02 -1.32690614e-01\n",
      "   1.66215249e-01 -1.37586033e-01  5.19207589e-02  2.51534363e-03\n",
      "   3.38038012e-02 -7.48271593e-02]\n",
      " [ 8.62065180e-01  1.79954568e-01  4.57276313e-02  5.48949724e-02\n",
      "   4.62793874e-02 -7.37804568e-02 -6.56912051e-03 -1.83892884e-01\n",
      "   3.88780283e-02 -1.43790432e-02]\n",
      " [ 8.39794476e-01  6.55314968e-02  6.47870819e-02 -8.64255768e-02\n",
      "   1.66328390e-01 -9.26797644e-02  3.01907508e-02 -1.49211630e-01\n",
      "  -1.16420795e-01  7.91626139e-03]\n",
      " [ 7.92948655e-01 -2.07717570e-01 -7.56519207e-02 -5.87951303e-02\n",
      "  -2.34052828e-04  1.68612693e-01 -1.24853055e-02 -1.12235319e-01\n",
      "   1.76606818e-01 -5.34760074e-02]\n",
      " [ 8.53558217e-01  1.14547672e-01  1.49839107e-01  1.29280736e-01\n",
      "   8.21172047e-04  1.25059857e-01  1.35627694e-02  6.39320316e-02\n",
      "  -5.20440749e-02 -1.28669175e-01]\n",
      " [ 7.88136326e-01 -7.58490955e-02  1.62019317e-01 -1.19000881e-02\n",
      "   1.18785081e-01 -8.94578740e-02 -1.79207152e-01  3.10151637e-01\n",
      "  -2.96237902e-02 -1.40284384e-01]\n",
      " [ 8.49687344e-01 -2.62706219e-01  1.37488900e-01  6.73662211e-02\n",
      "   6.94629303e-02  1.09780751e-01  6.08391533e-02  3.64834450e-02\n",
      "   5.27081820e-02  2.18515792e-02]\n",
      " [ 9.05346085e-01 -7.85518802e-02  1.75127127e-01  1.45251348e-02\n",
      "   1.18113355e-02  1.96080997e-02  3.01626163e-02  1.33715551e-02\n",
      "  -7.46912454e-02  2.43635925e-02]\n",
      " [ 8.55989955e-01 -7.60648300e-02  4.01483242e-02  9.19813548e-02\n",
      "  -2.69707966e-02  4.25167021e-02  4.60716473e-02 -2.61309831e-02\n",
      "  -1.37849854e-02 -1.63892752e-01]\n",
      " [ 9.47042431e-01 -4.32544502e-02  4.87734687e-02  1.30888718e-02\n",
      "  -5.78808670e-03 -3.03539443e-02 -4.19005809e-02 -2.04878804e-02\n",
      "   3.03554087e-02 -2.71635665e-02]\n",
      " [ 8.17709125e-01  3.04826349e-01 -2.55698710e-03  2.64502182e-01\n",
      "   4.79884611e-04 -1.94607866e-02 -4.62145116e-02  2.07244489e-02\n",
      "  -2.49245635e-02 -6.18854996e-02]\n",
      " [ 8.35819255e-01  1.82138676e-01 -9.52650754e-02  4.61986254e-02\n",
      "   1.01603201e-01 -1.00569960e-01 -8.13777028e-02  1.74875533e-01\n",
      "   4.02611497e-02  1.53049527e-03]\n",
      " [ 8.93420076e-01  8.81169943e-02  5.81206543e-03 -3.70599235e-03\n",
      "  -1.31669013e-03 -6.78444832e-02  1.01122575e-02 -2.43785667e-02\n",
      "  -4.64857200e-02 -3.03216041e-02]\n",
      " [ 8.84741182e-01 -5.55761988e-02  1.58330462e-02  7.04320324e-02\n",
      "  -7.50220950e-02  6.43707877e-02 -4.18158028e-02  4.85062675e-02\n",
      "   1.16065404e-01 -1.69405769e-02]\n",
      " [ 8.66816347e-01 -1.49486319e-01 -1.42145963e-02  1.75563921e-02\n",
      "  -7.97998625e-02  1.85172628e-02 -5.23021394e-02  1.04186312e-01\n",
      "   1.16563745e-01 -7.42302500e-02]\n",
      " [ 8.83477453e-01  1.16034613e-01  1.23811567e-02  2.59415212e-02\n",
      "  -8.28372757e-02 -2.81744135e-02  8.34543405e-02 -6.59598804e-02\n",
      "  -8.23832680e-02 -7.31814692e-02]\n",
      " [ 8.77877751e-01 -9.35915330e-02 -5.66490181e-03 -4.12193020e-02\n",
      "  -9.32243943e-02 -6.53865014e-02  8.54248504e-02  3.74033633e-02\n",
      "  -2.78671822e-02 -1.05152345e-01]\n",
      " [ 9.00708923e-01  1.20506168e-01 -1.09066442e-01 -8.98065145e-02\n",
      "  -6.66078115e-02 -1.74045501e-02 -6.29225206e-02  2.12756219e-02\n",
      "  -2.44955023e-02 -8.77673000e-03]\n",
      " [ 8.36911034e-01 -8.36180507e-02 -1.34739090e-01 -6.14744704e-02\n",
      "   3.79994476e-02  8.92855337e-02 -3.31582516e-02 -1.78309950e-02\n",
      "  -8.81525536e-02 -1.14270261e-01]\n",
      " [ 8.21093252e-01 -2.24875830e-01  6.21656585e-03  1.80515644e-01\n",
      "   2.39005548e-01 -5.99815634e-02  2.43151581e-02  5.31547445e-02\n",
      "  -6.21074792e-02  6.70281861e-02]\n",
      " [ 8.69609725e-01 -9.15060627e-02 -5.01455479e-02  1.40238944e-01\n",
      "   1.00093799e-01  1.29169617e-02 -1.42011689e-01  3.81831563e-04\n",
      "   4.54564440e-02 -2.36305142e-02]\n",
      " [ 8.35522220e-01 -2.75537130e-01  7.97919593e-02  1.84797391e-01\n",
      "   1.16400913e-01 -7.78310411e-04  1.31831224e-02  1.40641037e-02\n",
      "   1.59749277e-02  1.01420317e-01]\n",
      " [ 8.33588177e-01  1.05409966e-01  2.65661055e-02  9.34617610e-02\n",
      "   4.00878523e-02 -8.29733873e-02 -1.67819623e-01 -1.19668554e-01\n",
      "  -1.22818660e-01  7.57691657e-03]\n",
      " [ 8.77453967e-01 -9.44911048e-02  5.43666643e-02 -1.15765092e-02\n",
      "  -6.44013773e-03 -6.39715380e-02 -1.25395491e-02  3.40645929e-02\n",
      "  -6.57394679e-02  1.49334090e-02]\n",
      " [ 8.76134191e-01  3.81254826e-03  9.98700520e-02 -7.03367021e-02\n",
      "  -4.80260296e-02 -1.67863255e-01 -1.98075498e-02 -1.99862960e-02\n",
      "  -1.03094608e-01  6.63011597e-02]\n",
      " [ 8.79212347e-01  2.68868446e-02  7.78849557e-02  9.87578942e-02\n",
      "  -1.07406804e-01 -1.06539348e-01  1.66546676e-02 -4.70340683e-02\n",
      "  -2.87227890e-02  1.57672593e-01]\n",
      " [ 8.96591112e-01 -1.32670911e-02  7.40545252e-02  1.63294594e-01\n",
      "  -1.51787439e-01  7.37473803e-03  6.82277769e-02 -4.39848814e-02\n",
      "  -5.22955929e-02  2.85047194e-02]\n",
      " [ 8.40534166e-01 -2.40639536e-01  7.43538929e-02  3.30545984e-03\n",
      "  -1.78262596e-01 -1.62474215e-01  9.54653358e-02  5.49849398e-02\n",
      "  -4.61987584e-02  1.10246510e-01]\n",
      " [ 8.46571564e-01  2.18025535e-02 -1.24654724e-01  1.39633883e-01\n",
      "  -1.51657823e-02  2.48500338e-02 -9.09887338e-02  1.38524818e-02\n",
      "   9.52222120e-02 -4.28922484e-02]\n",
      " [ 8.25249019e-01 -1.68253722e-02 -6.55770721e-02  1.40691838e-01\n",
      "   1.05984621e-01  3.12162564e-02 -7.10577619e-02 -2.66135892e-04\n",
      "  -8.24715565e-03  1.36559308e-01]\n",
      " [ 8.08209133e-01  8.61870134e-03 -1.20705683e-01  1.14428358e-02\n",
      "   2.30185370e-01  2.44710992e-01  1.62436974e-01 -6.83895949e-03\n",
      "  -1.89871129e-01  1.15655740e-01]\n",
      " [ 8.65693274e-01  1.49636267e-01 -4.02775369e-02 -3.71082881e-02\n",
      "  -8.39380629e-02 -1.22519913e-02  1.17881144e-01 -4.21737712e-02\n",
      "  -1.10867708e-01 -3.76350539e-02]\n",
      " [ 8.84034650e-01 -1.29997277e-01 -8.67271086e-02 -2.18180485e-02\n",
      "   1.96584019e-02 -5.35818337e-02  6.69681783e-02  6.42313757e-02\n",
      "  -5.76273242e-02 -1.64138364e-04]\n",
      " [ 8.56017981e-01 -2.19419358e-01 -1.29029397e-01  5.44709515e-02\n",
      "  -4.09438502e-02  2.92285115e-02  7.95055788e-02 -1.88792714e-02\n",
      "   1.01510379e-02 -3.88195622e-02]\n",
      " [ 9.11121706e-01 -1.98244085e-01  3.86862440e-02 -1.69798389e-02\n",
      "  -1.44395568e-02 -1.58332297e-02  6.60976881e-02 -4.01414039e-03\n",
      "  -2.78751010e-02  5.73977903e-02]\n",
      " [ 8.93079552e-01 -2.42892639e-01 -5.96086646e-02 -7.61943680e-02\n",
      "  -7.98210792e-02 -5.03007654e-02 -4.53321843e-03 -3.40571612e-02\n",
      "   3.83631031e-02  2.35611038e-02]\n",
      " [ 8.53470898e-01 -2.19499053e-01 -8.60092286e-02 -1.10810566e-01\n",
      "   3.82985329e-02 -1.39182492e-01 -2.84131870e-02 -8.28278581e-02\n",
      "   1.97298829e-02  9.28241245e-03]\n",
      " [ 8.85150450e-01  4.39386009e-02 -1.40777063e-01 -9.88430648e-02\n",
      "  -8.22807241e-02  6.12854979e-02  6.30958863e-03  7.48750699e-02\n",
      "  -1.33289270e-01  2.66976909e-02]\n",
      " [ 9.29652037e-01  4.93844775e-02 -4.53743152e-02 -4.19217693e-02\n",
      "  -4.33221772e-02  7.28332569e-02  5.30225793e-02  3.63965976e-02\n",
      "  -1.22255339e-01 -1.40663737e-02]\n",
      " [ 8.88635952e-01  1.44531102e-02  1.06316977e-02  5.40682851e-03\n",
      "  -1.92071968e-01  1.17735858e-01  9.39525084e-03  7.11220797e-02\n",
      "  -4.59158835e-02 -3.79433419e-02]\n",
      " [ 8.40644675e-01  2.06947863e-01 -1.47736172e-01 -9.93735324e-02\n",
      "   1.25120261e-02  1.46495599e-01  1.47378566e-02  1.24355323e-01\n",
      "  -1.04999657e-01 -3.39948669e-02]\n",
      " [ 8.89317868e-01 -1.48579731e-01 -2.10774584e-03 -2.91539665e-02\n",
      "  -1.31262766e-01  3.79132284e-02  5.62017346e-02  1.05112633e-01\n",
      "   3.17382737e-02  2.16576038e-02]\n",
      " [ 8.84894005e-01 -2.00035969e-03 -1.20586024e-01 -6.54202295e-02\n",
      "   5.07719331e-02  1.71240011e-01  2.71913452e-03 -2.87891372e-02\n",
      "   3.02558343e-03  9.81701068e-02]\n",
      " [ 9.00982642e-01  4.06194679e-02  3.15738507e-02 -1.19292866e-01\n",
      "  -1.47592040e-01  7.91991449e-02 -4.97371279e-02  1.72985029e-02\n",
      "   6.10730280e-03  1.42483155e-01]\n",
      " [ 9.17560636e-01  5.08044323e-02 -3.19846198e-02 -1.54751526e-01\n",
      "  -6.34907434e-02 -2.09760495e-02 -2.71172056e-02  5.18672386e-02\n",
      "   3.94331403e-02  1.64587098e-02]\n",
      " [ 8.78535379e-01 -2.47165737e-02 -5.42041652e-02 -1.22980244e-01\n",
      "   4.20791146e-02 -4.18627968e-03 -1.04790479e-01 -3.41823285e-03\n",
      "   5.02244781e-02  4.93495147e-02]\n",
      " [ 8.98904355e-01  1.13894254e-02 -6.20238267e-02 -1.17446654e-01\n",
      "   1.66344308e-02  5.71807124e-02 -3.58484709e-02  2.21106366e-02\n",
      "   8.67106334e-02  4.04285294e-02]\n",
      " [ 8.68120722e-01  2.51729628e-01 -3.68529278e-02  1.53802102e-01\n",
      "   3.38586501e-02  1.36126503e-02 -9.20754090e-02 -3.39997617e-02\n",
      "  -2.61541247e-03 -5.66440973e-03]\n",
      " [ 8.66186560e-01  2.13555526e-02  2.83963450e-02  9.20590224e-02\n",
      "  -1.25632287e-01 -8.44513412e-02 -2.87459433e-02  6.63967736e-02\n",
      "  -1.99666022e-02  1.17606641e-03]\n",
      " [ 8.67178857e-01  1.99549308e-01 -1.21284543e-02  6.28200950e-02\n",
      "  -1.87502135e-01  3.51752713e-02 -4.96914769e-02 -1.12712945e-01\n",
      "   4.54998962e-02  1.15328692e-01]\n",
      " [ 8.73231912e-01  1.95301655e-01 -7.25610810e-02 -1.47996887e-01\n",
      "   4.35933964e-03 -9.13934603e-02 -7.08260616e-03  4.49318266e-02\n",
      "   4.33728475e-02  1.09289220e-01]\n",
      " [ 8.48220104e-01  1.95602830e-01 -9.72237380e-02  1.16622992e-01\n",
      "  -6.24153458e-02  6.70033579e-02 -1.37245194e-01 -1.23368436e-01\n",
      "   1.47144822e-01  8.34167457e-02]\n",
      " [ 8.41207458e-01 -1.73956126e-01 -1.61856222e-01 -3.52015402e-02\n",
      "  -7.65348055e-02 -8.95705645e-02  6.67791147e-03 -1.43036729e-02\n",
      "   1.48569321e-01 -8.62001361e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicialització del vectoritzador TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorització de la columna 'Content' de la taula normalized per obtenir la matriu term-document\n",
    "matriu_term_document = vectorizer.fit_transform(normalized['Content'])\n",
    "\n",
    "# Inicialització de l'algoritme LSA\n",
    "lsa = TruncatedSVD(10)\n",
    "\n",
    "# Aplicació de LSA a la matriu term-document\n",
    "lsa_result = lsa.fit_transform(matriu_term_document)\n",
    "\n",
    "# Mostra els resultats de LSA\n",
    "print(\"Resultats de LSA:\")\n",
    "print(lsa_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9a1a2225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<61x7670 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 41260 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriu_term_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3899f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f6add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fa1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Realitzar mineria de texts.\n",
    "Interpretació dels textos.\n",
    "Anàlisis dels resultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f67d1",
   "metadata": {},
   "source": [
    "## Exercici 1\n",
    "Agafa un text en anglès que vulguis, i calcula'n la freqüència de les paraules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02391bd",
   "metadata": {},
   "source": [
    "    He pujat el pdf d'un llibre (Orgull i prejudici)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6504c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creació del DataFrame de freqüències normalitzades\n",
    "df_frequencies = pd.DataFrame(matriu_term_document.toarray(), columns=termes)\n",
    "\n",
    "# Aplicació de LSA a la matriu de freqüències normalitzades\n",
    "lsa_result = lsa.fit_transform(df_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fcd84e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>is</th>\n",
       "      <th>truth</th>\n",
       "      <th>universally</th>\n",
       "      <th>acknowledged</th>\n",
       "      <th>that</th>\n",
       "      <th>single</th>\n",
       "      <th>man</th>\n",
       "      <th>in</th>\n",
       "      <th>posses</th>\n",
       "      <th>...</th>\n",
       "      <th>bath</th>\n",
       "      <th>heretofore</th>\n",
       "      <th>arrear</th>\n",
       "      <th>sportive</th>\n",
       "      <th>prehend</th>\n",
       "      <th>liberties</th>\n",
       "      <th>indignant</th>\n",
       "      <th>pollution</th>\n",
       "      <th>city</th>\n",
       "      <th>uniting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 7670 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     it   is  truth  universally  acknowledged  that  single  man   in  \\\n",
       "0   0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "1   0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "2   0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "3   0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "4   0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "..  ...  ...    ...          ...           ...   ...     ...  ...  ...   \n",
       "56  0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "57  0.0  0.0    0.0          0.0      0.013972   0.0     0.0  0.0  0.0   \n",
       "58  0.0  0.0    0.0          0.0      0.014261   0.0     0.0  0.0  0.0   \n",
       "59  0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "60  0.0  0.0    0.0          0.0      0.000000   0.0     0.0  0.0  0.0   \n",
       "\n",
       "      posses  ...     bath  heretofore  arrear  sportive   prehend  liberties  \\\n",
       "0   0.000000  ...  0.00000         0.0     0.0       0.0  0.000000   0.047707   \n",
       "1   0.000000  ...  0.00000         0.0     0.0       0.0  0.000000   0.000000   \n",
       "2   0.007975  ...  0.00000         0.0     0.0       0.0  0.000000   0.000000   \n",
       "3   0.000000  ...  0.00000         0.0     0.0       0.0  0.000000   0.000000   \n",
       "4   0.000000  ...  0.00000         0.0     0.0       0.0  0.000000   0.000000   \n",
       "..       ...  ...      ...         ...     ...       ...       ...        ...   \n",
       "56  0.000000  ...  0.00000         0.0     0.0       0.0  0.000000   0.000000   \n",
       "57  0.012050  ...  0.00000         0.0     0.0       0.0  0.000000   0.000000   \n",
       "58  0.012299  ...  0.00000         0.0     0.0       0.0  0.018958   0.000000   \n",
       "59  0.000000  ...  0.00000         0.0     0.0       0.0  0.000000   0.000000   \n",
       "60  0.030679  ...  0.02167         0.0     0.0       0.0  0.000000   0.000000   \n",
       "\n",
       "    indignant  pollution  city  uniting  \n",
       "0    0.000000   0.000000   0.0      0.0  \n",
       "1    0.000000   0.000000   0.0      0.0  \n",
       "2    0.000000   0.000000   0.0      0.0  \n",
       "3    0.000000   0.000000   0.0      0.0  \n",
       "4    0.000000   0.044617   0.0      0.0  \n",
       "..        ...        ...   ...      ...  \n",
       "56   0.000000   0.000000   0.0      0.0  \n",
       "57   0.000000   0.000000   0.0      0.0  \n",
       "58   0.000000   0.000000   0.0      0.0  \n",
       "59   0.025763   0.000000   0.0      0.0  \n",
       "60   0.000000   0.000000   0.0      0.0  \n",
       "\n",
       "[61 rows x 7670 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fda66a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it</th>\n",
       "      <th>is</th>\n",
       "      <th>truth</th>\n",
       "      <th>universally</th>\n",
       "      <th>acknowledged</th>\n",
       "      <th>that</th>\n",
       "      <th>single</th>\n",
       "      <th>man</th>\n",
       "      <th>in</th>\n",
       "      <th>posses</th>\n",
       "      <th>...</th>\n",
       "      <th>bath</th>\n",
       "      <th>heretofore</th>\n",
       "      <th>arrear</th>\n",
       "      <th>sportive</th>\n",
       "      <th>prehend</th>\n",
       "      <th>liberties</th>\n",
       "      <th>indignant</th>\n",
       "      <th>pollution</th>\n",
       "      <th>city</th>\n",
       "      <th>uniting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.005713</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.002878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.019534</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.018053</td>\n",
       "      <td>0.014557</td>\n",
       "      <td>0.018905</td>\n",
       "      <td>0.027965</td>\n",
       "      <td>0.031379</td>\n",
       "      <td>0.038771</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>0.030679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045178</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>0.031379</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>0.047707</td>\n",
       "      <td>0.025763</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.023285</td>\n",
       "      <td>0.022480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 7670 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              it         is      truth  universally  acknowledged       that  \\\n",
       "count  61.000000  61.000000  61.000000    61.000000     61.000000  61.000000   \n",
       "mean    0.000320   0.000358   0.001617     0.000239      0.001213   0.000458   \n",
       "std     0.002501   0.002797   0.004651     0.001864      0.004152   0.003581   \n",
       "min     0.000000   0.000000   0.000000     0.000000      0.000000   0.000000   \n",
       "25%     0.000000   0.000000   0.000000     0.000000      0.000000   0.000000   \n",
       "50%     0.000000   0.000000   0.000000     0.000000      0.000000   0.000000   \n",
       "75%     0.000000   0.000000   0.000000     0.000000      0.000000   0.000000   \n",
       "max     0.019534   0.021842   0.018053     0.014557      0.018905   0.027965   \n",
       "\n",
       "          single        man         in     posses  ...       bath  heretofore  \\\n",
       "count  61.000000  61.000000  61.000000  61.000000  ...  61.000000   61.000000   \n",
       "mean    0.000514   0.000636   0.001446   0.007484  ...   0.001961    0.000144   \n",
       "std     0.004018   0.004964   0.004909   0.007362  ...   0.006894    0.001121   \n",
       "min     0.000000   0.000000   0.000000   0.000000  ...   0.000000    0.000000   \n",
       "25%     0.000000   0.000000   0.000000   0.000000  ...   0.000000    0.000000   \n",
       "50%     0.000000   0.000000   0.000000   0.007292  ...   0.000000    0.000000   \n",
       "75%     0.000000   0.000000   0.000000   0.011359  ...   0.000000    0.000000   \n",
       "max     0.031379   0.038771   0.020305   0.030679  ...   0.045178    0.008755   \n",
       "\n",
       "          arrear   sportive    prehend  liberties  indignant  pollution  \\\n",
       "count  61.000000  61.000000  61.000000  61.000000  61.000000  61.000000   \n",
       "mean    0.000310   0.000514   0.000311   0.000782   0.000422   0.000731   \n",
       "std     0.002419   0.004018   0.002427   0.006108   0.003299   0.005713   \n",
       "min     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "25%     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "50%     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "75%     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "max     0.018893   0.031379   0.018958   0.047707   0.025763   0.044617   \n",
       "\n",
       "            city    uniting  \n",
       "count  61.000000  61.000000  \n",
       "mean    0.000382   0.000369  \n",
       "std     0.002981   0.002878  \n",
       "min     0.000000   0.000000  \n",
       "25%     0.000000   0.000000  \n",
       "50%     0.000000   0.000000  \n",
       "75%     0.000000   0.000000  \n",
       "max     0.023285   0.022480  \n",
       "\n",
       "[8 rows x 7670 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frequencies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bdd41",
   "metadata": {},
   "source": [
    "Obtenim que hi han 7670 paraules distintes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7265ed3",
   "metadata": {},
   "source": [
    "# Exercici 2\n",
    "Treu les stopwords i realitza stemming al teu conjunt de dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0f4059d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patricia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Patricia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Patricia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar las stopwords y el léxico en inglés si no están descargados\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "# Crear función preprocess_text\n",
    "def preprocess_text(text):\n",
    "    # Tokenización de palabras\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filtrado de tokens eliminando stopwords y palabras no reconocidas en inglés\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english') and token in english_words]\n",
    "    \n",
    "    # Lemmatización de tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Reconstrucción del texto procesado\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "96b5378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text and remove common words\n",
    "processed = normalized['Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9b68ae01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfiltered_tokens\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a95f944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "processed = normalized['Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429dd92",
   "metadata": {},
   "source": [
    "## Exercici 3\n",
    "Realitza sentiment analysis al teu conjunt de dades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc66e5",
   "metadata": {},
   "source": [
    "Abans de fer anàlisi de sentiment en de saber-hi què estem buscant, hem de fer una matriu term-document, què en aquest cas seria un array perquè no més tenim un document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Obre el fitxer PDF en mode de lectura binària\n",
    "with open('C:/Users/Patricia/Desktop/prideandprejudice.pdf', 'rb') as file:\n",
    "    # Crea un objecte de lectura PDF\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Llegeix cada pàgina i extreu'n el text\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    # Tokenitza el text en paraules\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Calcula la distribució de freqüència de les paraules\n",
    "    fdist = FreqDist(tokens)\n",
    "\n",
    "    # Obté les 10 paraules més freqüents relacionades amb \"amor\"\n",
    "    keywords = [word for word, freq in fdist.most_common(10) if 'love' in word or 'affection' in word]\n",
    "\n",
    "    # Imprimeix les paraules clau\n",
    "    print(\"Paraules clau relacionades amb 'amor':\")\n",
    "    for keyword in keywords:\n",
    "        print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9c81bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriu term-document:\n",
      "[[9.4581235e-05 9.4581235e-05 9.4581235e-05 ... 9.4581235e-05\n",
      "  9.4581235e-05 1.8916247e-04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicialització del vectoritzador TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorització del document per obtenir la matriu term-document\n",
    "matriu_term_document = vectorizer.fit_transform([text])\n",
    "\n",
    "# Llista de termes (columnes) de la matriu term-document\n",
    "termes = list(vectorizer.vocabulary_.keys())\n",
    "\n",
    "# Mostra la matriu term-document\n",
    "print(\"Matriu term-document:\")\n",
    "print(matriu_term_document.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b05afdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriu de termes i conceptes:\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Nombre de components per a l'LSA\n",
    "num_components = 2\n",
    "\n",
    "# Aplicació de LSA\n",
    "lsa = TruncatedSVD(n_components=num_components)\n",
    "lsa.fit(matriu_term_document.reshape(1, -1))\n",
    "\n",
    "# Matriu de termes i conceptes\n",
    "matriu_termes_conceptes = lsa.transform(matriu_term_document.reshape(1, -1))\n",
    "\n",
    "# Mostra la matriu de termes i conceptes\n",
    "print(\"Matriu de termes i conceptes:\")\n",
    "print(matriu_termes_conceptes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8115dfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriu_termes_conceptes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc21e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Patricia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Descarrega els recursos necessaris per a l'anàlisi de sentiment\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Inicialitza l'analitzador de sentiment\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b596f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_category(text):\n",
    "    positive_keywords = ['sympathy', 'affection', 'love']\n",
    "    negative_keywords = ['hate', 'disgust', 'repulsion']\n",
    "\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_category = 'Love'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_category = 'Hate'\n",
    "    else:\n",
    "        tokenized_text = nltk.word_tokenize(text.lower())\n",
    "        if any(keyword in tokenized_text for keyword in positive_keywords):\n",
    "            sentiment_category = 'Sympathy'\n",
    "        elif any(keyword in tokenized_text for keyword in negative_keywords):\n",
    "            sentiment_category = 'Hate'\n",
    "        else:\n",
    "            sentiment_category = 'Neutral'\n",
    "\n",
    "    return sentiment_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a18cfdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_category(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb89a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
